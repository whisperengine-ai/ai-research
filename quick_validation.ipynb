{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47941786",
   "metadata": {},
   "source": [
    "# Quick Validation: Consciousness Simulator Pipeline\n",
    "\n",
    "End-to-end validation of the consciousness simulator research infrastructure.\n",
    "\n",
    "**Goals:**\n",
    "1. Run a few conversations (2-3 scenarios)\n",
    "2. Generate metrics and interaction dynamics\n",
    "3. Create dashboard visualizations\n",
    "4. Validate data quality\n",
    "5. Check reliability/validity framework\n",
    "\n",
    "**Expected time:** ~5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721e9c5",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path\n",
    "project_path = Path('.').absolute()\n",
    "print(f\"Project path: {project_path}\")\n",
    "\n",
    "# Import our modules\n",
    "from run_conversation_test import ConversationTestRunner, SCENARIOS\n",
    "from research_dashboard import ResearchDashboard\n",
    "from reliability_validity import MetricValidator\n",
    "\n",
    "print(\"âœ… All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89439799",
   "metadata": {},
   "source": [
    "## 2. Run Conversations (Quick Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138da3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run just 3 quick scenarios to validate pipeline\n",
    "runner = ConversationTestRunner(verbose=True)\n",
    "\n",
    "test_scenarios = {\n",
    "    'Frustration to Hope': SCENARIOS['User Recovery from Frustration'],\n",
    "    'Anxiety to Confidence': SCENARIOS['Anxiety Management'],\n",
    "    'Positive Momentum': SCENARIOS['Positive Momentum']\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸš€ Running {len(test_scenarios)} test scenarios...\\n\")\n",
    "results = runner.run_batch(test_scenarios)\n",
    "\n",
    "print(f\"\\nâœ… Completed {len(runner.conversations)} conversations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1597978",
   "metadata": {},
   "source": [
    "## 3. Export & Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "json_file = runner.export_json('validation_results.json')\n",
    "csv_file = runner.export_csv('validation_summary.csv')\n",
    "\n",
    "print(f\"\\nðŸ“Š Results exported:\")\n",
    "print(f\"  JSON: {json_file}\")\n",
    "print(f\"  CSV: {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc450126",
   "metadata": {},
   "source": [
    "## 4. Generate Dashboard Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e587da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and generate dashboards\n",
    "dashboard = ResearchDashboard()\n",
    "num_convs = dashboard.load_json('validation_results.json')\n",
    "\n",
    "print(f\"Loaded {num_convs} conversations\")\n",
    "\n",
    "if num_convs > 0:\n",
    "    print(\"\\nðŸ“ˆ Generating visualizations...\")\n",
    "    outputs = dashboard.plot_all('validation_output')\n",
    "    \n",
    "    print(f\"\\nâœ… Generated {len(outputs)} visualizations:\")\n",
    "    for name, path in outputs.items():\n",
    "        print(f\"  â€¢ {name}: {path}\")\n",
    "else:\n",
    "    print(\"No data to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77565e50",
   "metadata": {},
   "source": [
    "## 5. Validate Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the results\n",
    "with open('validation_results.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "conversations = data.get('conversations', [])\n",
    "print(f\"\\nðŸ“‹ VALIDATION DATA QUALITY:\\n\")\n",
    "print(f\"Total conversations: {len(conversations)}\")\n",
    "\n",
    "# Check completeness\n",
    "complete = 0\n",
    "for conv in conversations:\n",
    "    if 'analysis' in conv and 'summary' in conv:\n",
    "        complete += 1\n",
    "\n",
    "print(f\"Complete records: {complete}/{len(conversations)} ({100*complete/len(conversations):.0f}%)\")\n",
    "\n",
    "# Sample metrics\n",
    "if conversations:\n",
    "    first_conv = conversations[0]\n",
    "    print(f\"\\nðŸ“Š Sample conversation: {first_conv['name']}\")\n",
    "    print(f\"  Turns: {first_conv['turns']}\")\n",
    "    \n",
    "    analysis = first_conv.get('analysis', {})\n",
    "    emotional = analysis.get('emotional_trajectory', {})\n",
    "    if emotional.get('status') == 'success':\n",
    "        print(f\"  Emotional trajectory: {emotional['start_emotion']} â†’ {emotional['end_emotion']}\")\n",
    "        print(f\"  Improvement: {emotional['trajectory']:.2f} ({emotional['direction'].upper()})\")\n",
    "    \n",
    "    resonance = analysis.get('resonance_analysis', {})\n",
    "    if resonance.get('status') == 'success':\n",
    "        print(f\"  Resonance: {resonance['average_resonance']:.1%}\")\n",
    "    \n",
    "    engagement = analysis.get('engagement_trajectory', {})\n",
    "    if engagement.get('status') == 'success':\n",
    "        print(f\"  Engagement: {engagement['average_engagement']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02999a1",
   "metadata": {},
   "source": [
    "## 6. Reliability & Validity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for validation\n",
    "improvements = []\n",
    "resonances = []\n",
    "engagements = []\n",
    "qualities = []\n",
    "\n",
    "for conv in conversations:\n",
    "    analysis = conv.get('analysis', {})\n",
    "    \n",
    "    emotional = analysis.get('emotional_trajectory', {})\n",
    "    if emotional.get('status') == 'success':\n",
    "        improvements.append(emotional.get('trajectory', 0))\n",
    "    \n",
    "    resonance = analysis.get('resonance_analysis', {})\n",
    "    if resonance.get('status') == 'success':\n",
    "        resonances.append(resonance.get('average_resonance', 0))\n",
    "    \n",
    "    engagement = analysis.get('engagement_trajectory', {})\n",
    "    if engagement.get('status') == 'success':\n",
    "        engagements.append(engagement.get('average_engagement', 0))\n",
    "    \n",
    "    appropriateness = analysis.get('appropriateness_analysis', {})\n",
    "    if appropriateness.get('status') == 'success':\n",
    "        qualities.append(appropriateness.get('average_appropriateness', 0))\n",
    "\n",
    "# Create metrics dict for validation\n",
    "metrics_dict = {\n",
    "    'improvement': improvements,\n",
    "    'resonance': resonances,\n",
    "    'engagement': engagements,\n",
    "    'quality': qualities\n",
    "}\n",
    "\n",
    "# Validate\n",
    "validator = MetricValidator()\n",
    "report = validator.generate_validity_report(metrics_dict)\n",
    "\n",
    "print(\"\\nðŸ“ˆ RELIABILITY & VALIDITY CHECK:\\n\")\n",
    "print(f\"Individual metrics analyzed: {len(report['individual_metrics'])}\")\n",
    "print(f\"Pairwise comparisons: {len(report['pairwise_comparisons'])}\")\n",
    "print(f\"\\nValidity Score: {report['summary']['validity_score']:.1%}\")\n",
    "print(f\"Assessment: {report['summary']['overall_assessment'].upper()}\")\n",
    "print(f\"\\nâœ… Data validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732ec68e",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda81205",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if improvements:\n",
    "    print(f\"\\nðŸ“Š Emotional Improvement (Mean Â± Std):\")\n",
    "    print(f\"   {np.mean(improvements):.2f} Â± {np.std(improvements):.2f}\")\n",
    "    print(f\"   Min/Max: {np.min(improvements):.2f} / {np.max(improvements):.2f}\")\n",
    "\n",
    "if resonances:\n",
    "    print(f\"\\nðŸ’“ Emotional Resonance (Mean Â± Std):\")\n",
    "    print(f\"   {np.mean(resonances):.1%} Â± {np.std(resonances):.1%}\")\n",
    "\n",
    "if engagements:\n",
    "    print(f\"\\nðŸŽ¯ User Engagement (Mean Â± Std):\")\n",
    "    print(f\"   {np.mean(engagements):.1%} Â± {np.std(engagements):.1%}\")\n",
    "\n",
    "if qualities:\n",
    "    print(f\"\\nâœ¨ Response Quality (Mean Â± Std):\")\n",
    "    print(f\"   {np.mean(qualities):.1%} Â± {np.std(qualities):.1%}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… PIPELINE VALIDATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nâœ“ {len(runner.conversations)} conversations executed\")\n",
    "print(f\"âœ“ Metrics extracted and validated\")\n",
    "print(f\"âœ“ Dashboards generated (validation_output/)\")\n",
    "print(f\"âœ“ Data exported (validation_results.json, validation_summary.csv)\")\n",
    "print(f\"\\nâ†’ Ready for ablation study or scaling!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
